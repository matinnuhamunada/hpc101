{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf83fac4",
   "metadata": {},
   "source": [
    "# CUDA \n",
    "* pip install cudatoolkit\n",
    "* Usual cuda operation in C/C++\n",
    "    * Move data from host memory from host to device memory.\n",
    "    * Launch the kernel with correct grid dimension.\n",
    "    * Execute kernel on the device.\n",
    "    * Move data from device memory to host memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9842127-83e2-4a8c-b3e0-2c3500369a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "from numba import jit, int32, prange, vectorize, float64, cuda\n",
    "import numpy as np\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788e2843-cff4-4264-a88e-2f98fafc0b88",
   "metadata": {},
   "source": [
    "## Kernel Function and Device Function\n",
    "In Numba, a Just-In-Time (JIT) compiler for Python that supports CUDA programming, the terms **kernel** and **device function** refer to different types of GPU functions that can be executed on CUDA-capable devices.\n",
    "\n",
    "### 1. **Kernel Function**\n",
    "\n",
    "- **Purpose**: Kernel functions are entry points to launch computations on the GPU. They are the top-level functions that are called from the host (CPU) and executed on the device (GPU).\n",
    "- **Execution**: A kernel function is executed by many parallel threads on the GPU, which are organized in grids and blocks.\n",
    "- **Calling**: Kernel functions are invoked from the host using a special syntax specifying the grid and block dimensions. In Numba, this is done using triple brackets `<<<grid_size, block_size>>>`.\n",
    "- **Return Value**: Kernel functions cannot return a value; instead, they work by modifying the input/output arrays passed as arguments.\n",
    "  \n",
    "\n",
    "### 2. **Device Function**\n",
    "\n",
    "- **Purpose**: Device functions are helper functions that run on the GPU and can only be called from other GPU functions (such as other device functions or kernels). They provide code reuse within the GPU.\n",
    "- **Execution**: Device functions are executed on the GPU as part of a kernel or another device function. They are not directly callable from the host (CPU).\n",
    "- **Calling**: Device functions are called like regular Python functions, but only from within other GPU functions.\n",
    "- **Return Value**: Unlike kernels, device functions can return values, making them useful for computations that need to be reused within the GPU code.\n",
    "  \n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "| **Aspect**            | **Kernel Function**                                | **Device Function**                                   |\n",
    "|-----------------------|----------------------------------------------------|-------------------------------------------------------|\n",
    "| **Callable From**      | Host (CPU)                                         | GPU (inside another kernel or device function)        |\n",
    "| **Return Value**       | No return values (modifies arguments)              | Can return values                                     |\n",
    "| **Launch Syntax**      | Launched with `<<<grid_size, block_size>>>` syntax | Called like a regular Python function                 |\n",
    "| **Purpose**            | Entry point for GPU computation                    | Reusable helper functions within GPU code             |\n",
    "| **Execution Context**  | Runs on the GPU, callable from the host            | Runs on the GPU, callable only from another GPU function|\n",
    "\n",
    "In summary, **kernel functions** are the primary interface for launching GPU computations from the CPU, while **device functions** are helper functions that run on the GPU and can be reused in kernels or other device functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e7c71c-5ffc-4d6c-b117-8268ed4b01ff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cdebf40",
   "metadata": {},
   "source": [
    "Numba automates the following:\n",
    "\n",
    "* Allcated GPU memory.\n",
    "* Copy data to the GPU memory.\n",
    "* Executed the CUDA kernel with the *correct kernel dimensions given the input sizes*.\n",
    "* Copy data to the host memory.\n",
    "* Return the result as a NumPy array. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aab0ae",
   "metadata": {},
   "source": [
    "* We can also create normal function that are called from a vectorized fucntion\n",
    "* Using cuda.jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2911795",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit(device=True) #This function will only be executed on a GPU\n",
    "def polar_to_cartesian(rho, theta):\n",
    "    x = rho * math.cos(theta)\n",
    "    y = rho * math.sin(theta)\n",
    "    return x, y  \n",
    "\n",
    "@vectorize(['float32(float32, float32, float32, float32)'], target='cuda')\n",
    "def polar_distance(rho1, theta1, rho2, theta2):\n",
    "    x1, y1 = polar_to_cartesian(rho1, theta1)\n",
    "    x2, y2 = polar_to_cartesian(rho2, theta2)\n",
    "    \n",
    "    return ((x1 - x2)**2 + (y1 - y2)**2)**0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5ac62e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/vp91/Training-Venv/intro-parallel-prog/lib/python3.11/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.2013700e-07,\n",
       "        0.0000000e+00, 2.9802322e-08, 0.0000000e+00, 2.1490760e-07,\n",
       "        0.0000000e+00, 1.8848644e-07, 1.6858739e-07, 2.3842040e-07,\n",
       "        0.0000000e+00, 2.7476383e-07, 2.3841858e-07, 5.3312016e-07,\n",
       "        0.0000000e+00, 6.7959752e-07, 5.9604645e-07, 7.7843413e-07,\n",
       "        2.3841858e-07, 9.5553514e-07, 5.9604935e-07, 4.2146849e-07,\n",
       "        4.7683716e-07, 4.9151248e-07, 1.1920929e-06, 4.2981520e-07,\n",
       "        3.3717478e-07, 5.1273855e-07, 5.3312016e-07, 8.5340866e-07,\n",
       "        0.0000000e+00, 1.4901397e-06, 1.1801118e-06, 2.5288108e-07,\n",
       "        1.9110703e-06, 8.3659103e-07, 1.7970578e-06, 1.1876141e-06,\n",
       "        4.7683716e-07, 1.6801348e-06, 9.5367432e-07, 5.6230908e-07,\n",
       "        1.6690798e-06, 1.2287812e-06, 1.1980385e-06, 1.7377591e-06,\n",
       "        9.5367432e-07, 1.6858739e-07, 6.3360608e-07, 1.7721735e-06,\n",
       "        3.5762787e-07, 1.5557270e-06, 2.6656008e-07, 1.1921060e-06,\n",
       "        1.3486991e-06, 9.5040912e-07, 1.3134171e-06, 1.4901161e-06,\n",
       "        1.5078915e-06, 4.5393546e-07, 1.9221920e-06, 5.0422324e-07,\n",
       "        0.0000000e+00, 7.9968021e-07, 5.9866005e-07, 1.6442674e-06,\n",
       "        1.7068173e-06, 1.3709271e-06, 4.9151248e-07, 8.4714117e-07,\n",
       "        2.0370492e-06, 2.6144639e-06, 4.9151248e-07, 3.1030215e-06,\n",
       "        1.5993604e-06, 3.2188254e-06, 3.3399881e-06, 3.4591242e-06,\n",
       "        9.6109602e-07, 7.7714947e-07, 1.5497208e-06, 8.8520926e-07,\n",
       "        1.6858739e-06, 3.1064544e-06, 6.9765446e-07, 2.0230486e-06,\n",
       "        4.8514664e-07, 3.0016133e-06, 2.5484035e-06, 1.3783574e-06,\n",
       "        3.8517701e-06, 1.5904503e-06, 1.5781167e-06, 2.3871642e-06,\n",
       "        4.7683716e-07, 1.5645510e-06, 3.9627002e-06, 1.0172157e-06]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(100, dtype='float32').reshape(1, 100)\n",
    "polar_distance(x, x, x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebd05bf-904f-4885-9f27-c82e27145b5f",
   "metadata": {},
   "source": [
    "\n",
    "## Thread Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "acb018f9-b9c2-4999-a585-f8cb2e715e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/vp91/Training-Venv/intro-parallel-prog/lib/python3.11/site-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def increment_a_2D_array(an_array):\n",
    "    x = cuda.threadIdx.x\n",
    "    y = cuda.threadIdx.y\n",
    "\n",
    "    # or x, y = cuda.grid(2)\n",
    "    \n",
    "    if x < an_array.shape[0] and y < an_array.shape[1]:\n",
    "        an_array[x, y] += 1\n",
    "\n",
    "an_array = np.random.rand(1000000)\n",
    "an_array = an_array.reshape(1000, 1000)\n",
    "\n",
    "threadsperblock = (16, 16)\n",
    "blockspergrid_x = math.ceil(an_array.shape[0] / threadsperblock[0])\n",
    "blockspergrid_y = math.ceil(an_array.shape[1] / threadsperblock[1])\n",
    "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "increment_a_2D_array[blockspergrid, threadsperblock](an_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff71b74e-2408-4c46-ad31-aaa6db8eb914",
   "metadata": {},
   "source": [
    "## Memory Management\n",
    "Data management is automatic in Numba, but we can also manage it manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c526f932-7cef-41d4-8b1c-dbdb9a96fa3a",
   "metadata": {},
   "source": [
    "### Host to device copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44e1613-20f8-47da-8c71-1a63dca02973",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cpu = np.arange(10)\n",
    "data_gpu = cuda.to_device(data_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d40807-ea96-4cc5-8f8f-f22eeee9b5b1",
   "metadata": {},
   "source": [
    "### Device to host copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea867e59-1fe3-4dca-8820-f66e5f0a4f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cpu = data_gpu.copy_to_host()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62351a2e-1359-44e9-af11-8a590dbae86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gpu.copy_to_host(data_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ece609-a7ab-48fd-b08e-fac39883b6d2",
   "metadata": {},
   "source": [
    "## Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e30e42e5-e249-4efa-b812-ad7009a022ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def add_kernel(a, b, c):\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.blockIdx.x\n",
    "    bw = cuda.blockDim.x\n",
    "\n",
    "    pos = tx + ty * bw\n",
    "\n",
    "    if pos < a.size:\n",
    "        c[pos] = a[pos] + b[pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "402469e8-13a0-479f-add8-55ee5962b625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two streams\n",
    "stream1 = cuda.stream()\n",
    "stream2 = cuda.stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0722d601-de90-4079-b916-eecc53c3eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data\n",
    "size = 1000000\n",
    "a_cpu = np.arange(size, dtype=np.float32)\n",
    "b_cpu = np.arange(size, dtype=np.float32) * 2\n",
    "c_cpu = np.zeros(size, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1783c38-ae29-4d95-a3a1-4696f3889a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate device memory\n",
    "a_gpu = cuda.to_device(a_cpu)\n",
    "b_gpu = cuda.to_device(b_cpu)\n",
    "c_gpu = cuda.device_array(size, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3256024-fc32-4a98-bd30-7fcb7eb205bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define block and grid dimensions\n",
    "threads_per_block = 256\n",
    "blocks_per_grid = (size + (threads_per_block - 1)) // threads_per_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17ebbc95-32af-4783-a4ec-56d874e1eb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch kernels in different streams\n",
    "add_kernel[blocks_per_grid, threads_per_block, stream1](a_gpu, b_gpu, c_gpu)\n",
    "add_kernel[blocks_per_grid, threads_per_block, stream2](b_gpu, c_gpu, a_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "868c89ef-78b9-4892-a71e-8b067e984b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the streams to complete\n",
    "stream1.synchronize()\n",
    "stream2.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0465455a-7d2e-411f-9770-b2f90f7da0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy result back to host\n",
    "c_cpu = c_gpu.copy_to_host()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf7c13a-64cf-420d-acc3-18f71ab4ea88",
   "metadata": {},
   "source": [
    "## Vectorization in GPU\n",
    "\n",
    "(n),()->(n) tells NumPy that the function takes a n-element one-dimension array, a scalar, denoted by the empty tuple (), and computes an n-element one-dimension array.\n",
    "\n",
    "Unlike vectorize() functions, guvectorize() functions should not return any result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f11e952-00dd-419b-a03e-b810f1e2466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import guvectorize, int64\n",
    "\n",
    "@guvectorize([(int64[:], int64, int64[:])], '(n),()->(n)')\n",
    "def g(x, y, res):\n",
    "    for i in range(x.shape[0]):\n",
    "        res[i] = x[i] + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e7b5375-b669-4609-85bc-95c82edee794",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = np.arange(size, dtype=np.int64)\n",
    "y = 10  # Scalar value\n",
    "\n",
    "# Invoke the function\n",
    "result = g(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7d737ac-a27d-47d5-84de-daaeb729df8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     10      11      12 ... 1000007 1000008 1000009]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5920b630-bae8-4193-8e48-6c81bbafd47c",
   "metadata": {},
   "source": [
    "In Numba's `@guvectorize` functions, there is no explicit return statement. Instead, the output is passed via the output argument (in this case, `res`). Numba modifies this array in place. When invoking the function, the result is automatically returned because Numba allocates an output array for you.\n",
    "\n",
    "\n",
    "### Key Points:\n",
    "1. **In-place Modification**: The `res` array is the output, which is modified in place within the `guvectorize` function.\n",
    "2. **Return**: When calling the `guvectorize`-decorated function, even though the function doesn't explicitly return anything, Numba provides the output array based on the function signature.\n",
    "\n",
    "\n",
    "\n",
    "### How the Return Works:\n",
    "\n",
    "- **`result = g(x, y)`**: Numba handles the allocation of the `res` array internally and returns it automatically after the function finishes.\n",
    "- You don't need to declare or pre-allocate the `res` array when calling the function; Numba will do this for you.\n",
    "  \n",
    "Thus, the array `result` contains the values produced by the in-place modification of `res` inside the `g` function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bc3df4-bef0-4ae0-b433-29c0466e9f62",
   "metadata": {},
   "source": [
    "## Reduction in GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2c4bc67-6b60-46e9-a052-16b878c2c7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.reduce\n",
    "def sum_reduce(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5278c5d-fd9d-46fc-a266-380b347477b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/vp91/Training-Venv/intro-parallel-prog/lib/python3.11/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 64 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/scratch/vp91/Training-Venv/intro-parallel-prog/lib/python3.11/site-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/scratch/vp91/Training-Venv/intro-parallel-prog/lib/python3.11/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "size = 1000000\n",
    "A = np.arange(size, dtype=np.int64)\n",
    "normal_sum = A.sum()      # NumPy sum reduction\n",
    "gpu_sum = sum_reduce(A)   # cuda sum reduction\n",
    "assert normal_sum == gpu_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651eeb5e-0194-47ba-89cc-71cea85bdffc",
   "metadata": {},
   "source": [
    "### What happens if you change the type to np.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c08fe2-fdcc-47e9-8126-f128b2a634d1",
   "metadata": {},
   "source": [
    "## Excercise 3\n",
    "[Go to Notebook Exercise 3](./Exercise3.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3fcf2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
